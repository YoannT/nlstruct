{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score mentions from tags\n",
    "\n",
    "The goal is to score a named entity recognition solution given the tags output by a sequence tagging model.\n",
    "The two main tagging schemes used nowadays are:\n",
    "- `BIO:   O O B-LOC I-LOC I-LOC O B-LOC O`\n",
    "- `BIOUL: O O B-LOC I-LOC L-LOC O U-LOC O`\n",
    "\n",
    "The function we code in this notebook returns the precision, recall and f1 scores by matching the predicted and gold sequences.\n",
    "Many overlapping schemes are available:\n",
    "- `exact`: mention pred and mention gold must match exactly\n",
    "- `partial_strict`: mention pred and mention gold must share at least one token\n",
    "- `partial`: mention pred and mention gold must touch\n",
    "- any scheme using a custom formula `begin_pred < end_gold and begin_gold < end_pred` to reproduce partial_strict for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from scipy.sparse import coo_matrix\n",
    "from itertools import zip_longest\n",
    "\n",
    "from nlstruct.layers.crf import BIODecoder, BIOULDecoder\n",
    "from nlstruct.core.batcher import factorize\n",
    "from nlstruct.core.scoring import compute_metrics, merge_pred_and_gold\n",
    "\n",
    "def score_mentions_from_tags(y_true, y_pred):\n",
    "    # Detect the tag scheme\n",
    "    tags = factorize(y_true, freeze_reference=False)[2]\n",
    "    heads, labels = tuple(zip_longest(*(tag.split(\"-\") for tag in tags)))\n",
    "    heads, labels = set(heads), np.asarray(list(set(label for label in labels if label is not None)))\n",
    "    if heads <= set(\"BIO\"):\n",
    "        tags = [\"O\"] + [tag for label in labels for tag in (f\"B-{label}\", f\"I-{label}\")]\n",
    "        decoder = BIODecoder # let's reuse the LinearCRF tag decoder\n",
    "    elif heads <= set(\"BIOUL\"):\n",
    "        tags = [\"O\"] + [tag for label in labels for tag in (f\"B-{label}\", f\"I-{label}\", f\"L-{label}\", f\"U-{label}\")]\n",
    "        decoder = BIOULDecoder # let's reuse the LinearCRF tag decoder\n",
    "    else:\n",
    "        raise Exception(\"Unrecognized tags {}. Allowed schemes are BIO and BIOUL\".format(tuple(heads)))\n",
    "    \n",
    "    # Transform all the string tags to numbers, according to the detected `tags` (the order matters !)  \n",
    "    y_true = factorize(y_true, reference_values=tags)[0]\n",
    "    y_pred = factorize(y_pred, reference_values=tags)[0]\n",
    "\n",
    "    # Convert the tags variable length sequences to matrices of size n_sequence * max_tokens_per_seq -> id of the tag\n",
    "    true_sparse_content = np.asarray([(row, col, val) for row, vals in enumerate(y_true) for col, val in enumerate(vals)])\n",
    "    pred_sparse_content = np.asarray([(row, col, val) for row, vals in enumerate(y_pred) for col, val in enumerate(vals)])\n",
    "    sp_true = coo_matrix((true_sparse_content[:, 2], (true_sparse_content[:, 0], true_sparse_content[:, 1])))\n",
    "    sp_pred = coo_matrix((pred_sparse_content[:, 2], (pred_sparse_content[:, 0], pred_sparse_content[:, 1])))\n",
    "    \n",
    "    # Use the BIO(UL)Decoder tags_to_spans method to extract begin/end indices, labels and doc_id of each span\n",
    "    true_spans = decoder.tags_to_spans(torch.as_tensor(sp_true.toarray()))\n",
    "    pred_spans = decoder.tags_to_spans(torch.as_tensor(sp_pred.toarray()))\n",
    "    \n",
    "    # Build dataframes using those computed indice/label arrays\n",
    "    pred=pd.DataFrame({\"begin\": true_spans[\"span_begin\"], \"end\": true_spans[\"span_end\"], \"doc_id\": true_spans[\"span_doc_id\"], \"label\": labels[true_spans[\"span_label\"]]})\n",
    "    gold=pd.DataFrame({\"begin\": pred_spans[\"span_begin\"], \"end\": pred_spans[\"span_end\"], \"doc_id\": pred_spans[\"span_doc_id\"], \"label\": labels[pred_spans[\"span_label\"]]})\n",
    "\n",
    "    # Compute the metrics, each compute_metrics function returns a dict\n",
    "    metrics = {\n",
    "        # True positive only when exact match and same label \n",
    "        **compute_metrics(merge_pred_and_gold(pred, gold,\n",
    "            span_policy='exact',  # exact match, could also write \"begin_x <= end_y or begin_y <= end_x\" equivalently\n",
    "            on=[\"doc_id\", (\"begin\", \"end\"), \"label\"]), prefix='exact/full/').to_dict(),\n",
    "\n",
    "        # True positive only when partial strict overlap and same label \n",
    "        **compute_metrics(merge_pred_and_gold(pred, gold,\n",
    "            span_policy='partial_strict',\n",
    "            on=[\"doc_id\", (\"begin\", \"end\"), \"label\"]), prefix='relaxed/full/').to_dict(),\n",
    "\n",
    "        # True positive only when exact match and the label maybe different\n",
    "        **compute_metrics(merge_pred_and_gold(pred, gold,\n",
    "            span_policy='exact',\n",
    "            on=[\"doc_id\", (\"begin\", \"end\")]), prefix='exact/span/').to_dict(),\n",
    "\n",
    "        # True positive only when partial strict overlap and the label maybe different\n",
    "        **compute_metrics(merge_pred_and_gold(pred, gold,\n",
    "            span_policy='partial_strict',\n",
    "            on=[\"doc_id\", (\"begin\", \"end\")]), prefix='relaxed/span/').to_dict(),\n",
    "    }\n",
    "    # Compute per-label scores\n",
    "    for label in gold['label'].drop_duplicates():\n",
    "        subset_pred = pred.query(f\"label == {repr(label)}\")\n",
    "        subset_gold = gold.query(f\"label == {repr(label)}\")\n",
    "        metrics.update(compute_metrics(merge_pred_and_gold(subset_pred, subset_gold,\n",
    "            span_policy='exact',\n",
    "            on=[\"doc_id\", (\"begin\", \"end\")]), prefix=f'exact/{label}/').to_dict(),)\n",
    "        metrics.update(compute_metrics(merge_pred_and_gold(subset_pred, subset_gold,\n",
    "            span_policy='partial_strict',\n",
    "            on=[\"doc_id\", (\"begin\", \"end\")]), prefix=f'relaxed/{label}/').to_dict(),)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_mentions_from_tags returns a dict\n",
    "metrics = score_mentions_from_tags(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gold_count</th>\n",
       "      <th>tp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">exact</th>\n",
       "      <th>MISC</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>span</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">relaxed</th>\n",
       "      <th>MISC</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>span</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               f1  precision  recall  pred_count  gold_count  tp\n",
       "exact   MISC  0.0        0.0     0.0           1           1   0\n",
       "        PER   1.0        1.0     1.0           1           1   1\n",
       "        full  0.5        0.5     0.5           2           2   1\n",
       "        span  0.5        0.5     0.5           2           2   1\n",
       "relaxed MISC  1.0        1.0     1.0           1           1   1\n",
       "        PER   1.0        1.0     1.0           1           1   1\n",
       "        full  1.0        1.0     1.0           2           2   2\n",
       "        span  1.0        1.0     1.0           2           2   2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the metrics using a pretty dataframe\n",
    "pd.Series({tuple(k.split(\"/\")): value for k, value in metrics.items()}).unstack(2)[\n",
    "    [\"f1\", \"precision\", \"recall\", \"pred_count\", \"gold_count\", \"tp\"]].astype({\"pred_count\": int, \"gold_count\": int, \"tp\": int})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlstruct",
   "language": "python",
   "name": "nlstruct"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
