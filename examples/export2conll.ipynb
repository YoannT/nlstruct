{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nlstruct.core.text import transform_text, apply_deltas, encode_as_tag, split_into_spans\n",
    "from nlstruct.core.pandas import merge_with_spans, make_id_from_merged\n",
    "from nlstruct.core.cache import get_cache\n",
    "from nlstruct.core.environment import env\n",
    "from nlstruct.chunking.spacy_tokenization import spacy_tokenize, SPACY_ATTRIBUTES\n",
    "\n",
    "# from nlstruct.dataloaders.ncbi_disease import load_ncbi_disease\n",
    "# from nlstruct.dataloaders.bc5cdr import load_bc5cdr\n",
    "from nlstruct.dataloaders.n2c2_2019_task3 import load_n2c2_2019_task3\n",
    "from nlstruct.dataloaders.brat import load_from_brat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(\n",
       "  (docs):        50 * ('doc_id', 'text', 'split')\n",
       "  (mentions):  6684 * ('doc_id', 'mention_id', 'label')\n",
       "  (fragments): 6792 * ('doc_id', 'mention_id', 'fragment_id', 'begin', 'end')\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset = docs, mentions, labels, fragments = load_ncbi_disease()[[\"docs\", \"mentions\", \"labels\", \"fragments\"]]\n",
    "# dataset = docs, mentions, labels, fragments = load_bc5cdr()[[\"docs\", \"mentions\", \"labels\", \"fragments\"]]\n",
    "# dataset = docs, mentions, fragments = load_from_brat(env.resource(\"brat/my_brat_dataset/\"))[[\"docs\", \"mentions\", \"fragments\"]]\n",
    "dataset = docs, mentions, fragments = load_n2c2_2019_task3()[[\"docs\", \"mentions\", \"fragments\"]]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform docs\n",
    "Apply substitutions to the documents and translate spans accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define subs as (\"pattern\", \"replacements\") list\n",
    "subs = [\n",
    "    (re.escape(\"<????-??-??>\"), \"MASKEDDATE\"),\n",
    "    (r\"(?<=[{}\\\\])(?![ ])\".format(string.punctuation), r\" \"),\n",
    "    (r\"(?<![ ])(?=[{}\\\\])\".format(string.punctuation), r\" \"),\n",
    "    (\"(?<=[a-zA-Z])(?=[0-9])\", r\" \"),\n",
    "    (\"(?<=[0-9])(?=[A-Za-z])\", r\" \"),\n",
    "    (\"MASKEDDATE\", \"<????-??-??>\"),\n",
    "]\n",
    "# Clean the text / perform substitutions\n",
    "docs, deltas = transform_text(docs, *zip(*subs), return_deltas=True)\n",
    "\n",
    "# Apply transformations to the spans\n",
    "fragments = apply_deltas(fragments, deltas, on='doc_id')\n",
    "fragments = fragments.merge(mentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the documents, and define fragments as spans of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "tokens = (\n",
    "    spacy_tokenize(docs, lang=\"en_core_web_sm\", spacy_attributes=[\"orth_\"])#, spacy_attributes=list((set(SPACY_ATTRIBUTES) - {\"norm_\"}) | {\"lemma_\"}),)\n",
    "    #spm_tokenize(docs, \"/Users/perceval/Development/data/resources/camembert.v0/sentencepiece.bpe.model\")\n",
    ")\n",
    "\n",
    "# Perform token substitution to match CoNLL guidelines\n",
    "tokens[\"token_orth\"] = tokens[\"token_orth\"].apply(lambda word: {\n",
    "    \"$\": \"${dollar}\",\n",
    "    \"_\": \"${underscore}\",\n",
    "    \"\\t\": \"${tab}\",\n",
    "    \"\\n\": \"${newline}\",\n",
    "    \" \": \"${space}\",\n",
    "    \"#\": \"${hash}\"}.get(word, word))\n",
    "\n",
    "tokenized_fragments = split_into_spans(fragments, tokens, pos_col=\"token_idx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract overlapping spans\n",
    "conflicts = merge_with_spans(tokenized_fragments, tokenized_fragments, on=[\"doc_id\", (\"begin\", \"end\")], how=\"outer\", suffixes=(\"\", \"_other\"))\n",
    "\n",
    "# Assign a cluster (overlapping fragments) to each fragment\n",
    "fragment_cluster_ids = make_id_from_merged(\n",
    "    conflicts[[\"doc_id\", \"mention_id\", \"fragment_id\"]], \n",
    "    conflicts[[\"doc_id\", \"mention_id_other\", \"fragment_id_other\"]], \n",
    "    apply_on=[(0, tokenized_fragments[[\"doc_id\", \"mention_id\", \"fragment_id\"]])])\n",
    "\n",
    "# Group by cluster and set the biggest fragment to depth 0, next to 1, ...\n",
    "split_fragments = (tokenized_fragments\n",
    " .groupby(fragment_cluster_ids, as_index=False, group_keys=False)\n",
    " .apply(lambda group: group.assign(depth=np.argsort(group[\"begin\"]-group[\"end\"]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode mentions as tags on tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6699/6699 [00:06<00:00, 1092.37it/s]\n",
      "100%|██████████| 89/89 [00:00<00:00, 1015.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>token_idx</th>\n",
       "      <th>token_orth</th>\n",
       "      <th>label-0</th>\n",
       "      <th>label-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0034</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>054478430</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0034</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>ELMVH</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0034</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>${newline}</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0034</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>79660638</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0034</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>${newline}</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc_id  token_id  begin  end  token_idx  token_orth label-0 label-1\n",
       "0   0034         0      0    9          0   054478430       O       O\n",
       "1   0034         1     10   15          1       ELMVH       O       O\n",
       "2   0034         2     15   16          2  ${newline}       O       O\n",
       "3   0034         3     16   24          3    79660638       O       O\n",
       "4   0034         4     24   25          4  ${newline}       O       O"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode labels into tag on tokens, with respect to the fragments indices\n",
    "tagged_tokens = tokens.copy()\n",
    "tag_scheme=\"bio\" # / \"bioul\"\n",
    "label_col_names = []\n",
    "for depth_i in range(split_fragments[\"depth\"].max()):\n",
    "    label_col_names.append(f'label-{depth_i}')\n",
    "    tagged_tokens[f'label-{depth_i}'] = encode_as_tag(tokens[[\"doc_id\", \"token_id\", \"token_idx\"]], \n",
    "                                                      split_fragments[split_fragments[\"depth\"] == depth_i], \n",
    "                                                      tag_scheme=tag_scheme, label_cols=[\"label\"], use_token_idx=True, verbose=1)[0]['label']\n",
    "tagged_tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the CoNLL files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [00:00<00:00, 107.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache /Users/perceval/Development/data/cache/n2c2_conll/4d8c0405832b0f7e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 85.26it/s] \n"
     ]
    }
   ],
   "source": [
    "# Alternatively, we could use the nlstruct.exporters.conll.to_conll function like so:\n",
    "# to_conll(\n",
    "#    dataset=Dataset(tokens=tagged_tokens, docs=docs), \n",
    "#    token_cols=[\"token_orth\", *label_col_names], \n",
    "#    destination=\"n2c2_conll\"\n",
    "# )\n",
    "\n",
    "cache = get_cache(\"n2c2_conll\")\n",
    "for doc_id, doc_tokens in tqdm(tagged_tokens.groupby([\"doc_id\"], sort=\"begin\")):\n",
    "    with open(cache / (doc_id + \".conll\"), \"w\") as file:\n",
    "        for (token_idx, token, *token_labels) in doc_tokens[[\"token_idx\", \"token_orth\", *label_col_names]].itertuples(index=False): # iter(zip(*df)) is way faster than df.iterrows()\n",
    "            print(token_idx, \"\\t\", token, \"\\t\", \"\\t\".join(token_labels), file=file)\n",
    "for doc_id, doc_text in docs[[\"doc_id\", \"text\"]].itertuples(index=False):\n",
    "    with open(cache / (doc_id + \".txt\"), \"w\") as file:\n",
    "        print(doc_text, file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLStruct",
   "language": "python",
   "name": "nlstruct"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
