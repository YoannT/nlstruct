{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nlstruct.core.text import transform_text, apply_deltas, encode_as_tag, split_into_spans\n",
    "from nlstruct.core.pandas import merge_with_spans, make_id_from_merged\n",
    "from nlstruct.core.cache import get_cache\n",
    "from nlstruct.core.environment import env\n",
    "from nlstruct.chunking.spacy_tokenization import spacy_tokenize, SPACY_ATTRIBUTES\n",
    "\n",
    "# from nlstruct.dataloaders.ncbi_disease import load_ncbi_disease\n",
    "# from nlstruct.dataloaders.bc5cdr import load_bc5cdr\n",
    "from nlstruct.dataloaders.n2c2_2019_task3 import load_n2c2_2019_task3\n",
    "from nlstruct.dataloaders.brat import load_from_brat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(\n",
       "  (docs):        50 * ('doc_id', 'text', 'split')\n",
       "  (mentions):  6684 * ('doc_id', 'mention_id', 'label')\n",
       "  (fragments): 6792 * ('doc_id', 'mention_id', 'fragment_id', 'begin', 'end')\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset = docs, mentions, labels, fragments = load_ncbi_disease()[[\"docs\", \"mentions\", \"labels\", \"fragments\"]]\n",
    "# dataset = docs, mentions, labels, fragments = load_bc5cdr()[[\"docs\", \"mentions\", \"labels\", \"fragments\"]]\n",
    "dataset = docs, mentions, fragments = load_n2c2_2019_task3()[[\"docs\", \"mentions\", \"fragments\"]]\n",
    "# dataset = docs, mentions, fragments = load_from_brat(env.resource(\"brat/my_brat_dataset/\"))[[\"docs\", \"mentions\", \"fragments\"]]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform docs\n",
    "Apply substitutions to the documents and translate spans accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define subs as (\"pattern\", \"replacements\") list\n",
    "subs = [\n",
    "    (re.escape(\"<????-??-??>\"), \"MASKEDDATE\"),\n",
    "    (r\"(?<=[{}\\\\])(?![ ])\".format(string.punctuation), r\" \"),\n",
    "    (r\"(?<![ ])(?=[{}\\\\])\".format(string.punctuation), r\" \"),\n",
    "    (\"(?<=[a-zA-Z])(?=[0-9])\", r\" \"),\n",
    "    (\"(?<=[0-9])(?=[A-Za-z])\", r\" \"),\n",
    "    (\"MASKEDDATE\", \"<????-??-??>\"),\n",
    "]\n",
    "# Clean the text / perform substitutions\n",
    "docs, deltas = transform_text.nocache(docs, *zip(*subs), return_deltas=True)\n",
    "\n",
    "# Apply transformations to the spans\n",
    "fragments = apply_deltas(fragments, deltas, on='doc_id')\n",
    "fragments = fragments.merge(mentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the documents, and define fragments as spans of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "tokens = (\n",
    "    spacy_tokenize.nocache(docs, lang=\"en_core_web_sm\", spacy_attributes=[\"orth_\"])#, spacy_attributes=list((set(SPACY_ATTRIBUTES) - {\"norm_\"}) | {\"lemma_\"}),)\n",
    "    #spm_tokenize.nocache(docs, \"/Users/perceval/Development/data/resources/camembert.v0/sentencepiece.bpe.model\")\n",
    ")\n",
    "\n",
    "# Perform token substitution to match CoNLL guidelines\n",
    "tokens[\"token_orth\"] = tokens[\"token_orth\"].apply(lambda word: {\n",
    "    \"$\": \"${dollar}\",\n",
    "    \"_\": \"${underscore}\",\n",
    "    \"\\t\": \"${tab}\",\n",
    "    \"\\n\": \"${newline}\",\n",
    "    \" \": \"${space}\",\n",
    "    \"#\": \"${hash}\"}.get(word, word))\n",
    "\n",
    "tokenized_fragments = split_into_spans(fragments, tokens, pos_col=\"token_idx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract overlapping spans\n",
    "conflicts = merge_with_spans(tokenized_fragments, tokenized_fragments, on=[\"doc_id\", (\"begin\", \"end\")], how=\"outer\", suffixes=(\"\", \"_other\"))\n",
    "\n",
    "# Assign a cluster (overlapping fragments) to each fragment\n",
    "fragment_cluster_ids = make_id_from_merged(\n",
    "    conflicts[[\"doc_id\", \"mention_id\", \"fragment_id\"]], \n",
    "    conflicts[[\"doc_id\", \"mention_id_other\", \"fragment_id_other\"]], \n",
    "    apply_on=[(0, tokenized_fragments[[\"doc_id\", \"mention_id\", \"fragment_id\"]])])\n",
    "\n",
    "# Group by cluster and set the biggest fragment to depth 0, next to 1, ...\n",
    "split_fragments = (tokenized_fragments\n",
    " .groupby(fragment_cluster_ids, as_index=False, group_keys=False)\n",
    " .apply(lambda group: group.assign(depth=np.argsort(group[\"begin\"]-group[\"end\"]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode mentions as tags on tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1569it [00:01, 1348.64it/s]                          \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>token_idx</th>\n",
       "      <th>token_orth</th>\n",
       "      <th>label-0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_-1040562575731457347_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>Hôpital</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_-1040562575731457347_1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>TENON</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_-1040562575731457347_1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>${newline}</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_-1040562575731457347_1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>Avenue</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_-1040562575731457347_1</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>Patricia</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       doc_id  token_id  begin  end  token_idx  token_orth  \\\n",
       "0  doc_-1040562575731457347_1         0      0    7          0     Hôpital   \n",
       "1  doc_-1040562575731457347_1         1      8   13          1       TENON   \n",
       "2  doc_-1040562575731457347_1         2     13   14          2  ${newline}   \n",
       "3  doc_-1040562575731457347_1         3     14   20          3      Avenue   \n",
       "4  doc_-1040562575731457347_1         4     21   29          4    Patricia   \n",
       "\n",
       "  label-0  \n",
       "0       O  \n",
       "1       O  \n",
       "2       O  \n",
       "3       O  \n",
       "4       O  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode labels into tag on tokens, with respect to the fragments indices\n",
    "tagged_tokens = tokens.copy()\n",
    "tag_scheme=\"bio\" # / \"bioul\"\n",
    "label_col_names = []\n",
    "for depth_i in range(split_fragments[\"depth\"].max()):\n",
    "    label_col_names.append(f'label-{depth_i}')\n",
    "    tagged_tokens[f'label-{depth_i}'] = encode_as_tag(tokens[[\"doc_id\", \"token_id\", \"token_idx\"]], \n",
    "                                                      split_fragments[split_fragments[\"depth\"] == depth_i], \n",
    "                                                      tag_scheme=tag_scheme, label_cols=[\"label\"], use_token_idx=True, verbose=1)['label']\n",
    "tagged_tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the CoNLL files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [00:00<00:00, 124.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache /Users/perceval/Development/data/cache/n2c2_conll/0eece35480718b75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 130.49it/s]\n"
     ]
    }
   ],
   "source": [
    "cache = get_cache(\"n2c2_conll\")\n",
    "for doc_id, doc_tokens in tqdm(tagged_tokens.groupby([\"doc_id\"], sort=\"begin\")):\n",
    "    with open(cache / (doc_id + \".conll\"), \"w\") as file:\n",
    "        for (token_idx, token, *token_labels) in doc_tokens[[\"token_idx\", \"token_orth\", *label_col_names]].itertuples(index=False): # iter(zip(*df)) is way faster than df.iterrows()\n",
    "            print(token_idx, \"\\t\", token, \"\\t\", \"\\t\".join(token_labels), file=file)\n",
    "for doc_id, doc_text in docs[[\"doc_id\", \"text\"]].itertuples(index=False):\n",
    "    with open(cache / (doc_id + \".txt\"), \"w\") as file:\n",
    "        print(doc_text, file=file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLStruct",
   "language": "python",
   "name": "nlstruct"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
