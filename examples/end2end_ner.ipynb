{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to end medical NER on NCBI\n",
    "\n",
    "We load NCBI dataset and perform medical mention detection on it (without the normalization of the mentions) using a token and character level neural network.\n",
    "\n",
    "Every component (except the CRF) needed to perform the NER tagging is coded in this notebook.\n",
    "Likewise, every preprocessing step is shown, and can be written by composing very few core functions into a few lines.\n",
    "Finally, we convert back predictions to character spans to be able to display the annotated documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache /Users/perceval/Development/data/cache/ncbi_raw_files/4d8c0405832b0f7e\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': Dataset(\n",
       "   (docs):       634 * ('doc_id', 'text', 'split')\n",
       "   (mentions):  5590 * ('doc_id', 'mention_id', 'category')\n",
       "   (labels):    5739 * ('doc_id', 'label_id', 'mention_id', 'label')\n",
       "   (fragments): 5590 * ('doc_id', 'mention_id', 'begin', 'end', 'fragment_id')\n",
       " ),\n",
       " 'test': Dataset(\n",
       "   (docs):       158 * ('doc_id', 'text', 'split')\n",
       "   (mentions):  1291 * ('doc_id', 'mention_id', 'category')\n",
       "   (labels):    1320 * ('doc_id', 'label_id', 'mention_id', 'label')\n",
       "   (fragments): 1291 * ('doc_id', 'mention_id', 'begin', 'end', 'fragment_id')\n",
       " )}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlstruct.dataloaders.ncbi_disease import load_ncbi_disease\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# A Dataset object is just a boosted dict of pandas DataFrames\n",
    "dataset = load_ncbi_disease()\n",
    "\n",
    "# Split into train / test\n",
    "test_split = 0.2\n",
    "splits = (\n",
    "    ([\"val\"] * int(len(dataset['docs']) * test_split)) + \n",
    "    ([\"train\"] * (len(dataset['docs']) - int(len(dataset['docs']) * test_split))))\n",
    "check_random_state(42).shuffle(splits)\n",
    "dataset['docs']['split'] = splits\n",
    "train_dataset = dataset.query(\"split == 'train'\", propagate=True)\n",
    "test_dataset = dataset.query(\"split == 'val'\", propagate=True)\n",
    "{\"train\": train_dataset, \"test\": test_dataset}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, mentions, fragments = train_dataset[[\"docs\", \"mentions\", \"fragments\"]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache /Users/perceval/Development/data/cache/nlstruct/core/text/transform_text/fb53b7431d13db40\n"
     ]
    }
   ],
   "source": [
    "from nlstruct.core.text import transform_text, apply_deltas\n",
    "from nlstruct.core.cache import cached\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Define subs as (\"pattern\", \"replacements\") list\n",
    "subs = [\n",
    "    (r\"(?<=[{}\\\\])(?![ ])\".format(string.punctuation), r\" \"),\n",
    "    (r\"(?<![ ])(?=[{}\\\\])\".format(string.punctuation), r\" \"),\n",
    "    (\"(?<=[a-zA-Z])(?=[0-9])\", r\" \"),\n",
    "    (\"(?<=[0-9])(?=[A-Za-z])\", r\" \"),\n",
    "]\n",
    "# Clean the text / perform substitutions\n",
    "# `deltas` contains the character span shifts made by the substitutions\n",
    "# we will reuse it at the end of the notebook to convert map predictions on input text\n",
    "docs, deltas = cached(transform_text)(docs, *zip(*subs), return_deltas=True)\n",
    "\n",
    "# Apply transformations to the spans\n",
    "fragments = apply_deltas(fragments, deltas, on='doc_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem reformulation\n",
    "Consider each fragment in a mention as a mention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each fragment is a mention\n",
    "mentions = mentions.merge(fragments)\n",
    "mentions[\"mention_id\"] = mentions[\"mention_id\"].astype(str) + '-' + mentions[\"fragment_id\"].astype(str)\n",
    "del mentions[\"fragment_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split docs into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlstruct.chunking.spacy_tokenization import sentencize\n",
    "from nlstruct.core.pandas import merge_with_spans, make_id_from_merged\n",
    "from nlstruct.core.text import partition_spans\n",
    "\n",
    "# Sentencize and make new docs from sentences\n",
    "sentences = sentencize(docs)\n",
    "[mentions], sentences, old_to_new_doc_mapper = partition_spans([mentions], sentences, new_id_name=\"doc_id\")\n",
    "docs = sentences.merge(old_to_new_doc_mapper).merge(docs.rename({\"doc_id\": \"_doc_id\"}, axis=1)).drop(columns=[\"_doc_id\"])\n",
    "docs[\"text\"] = docs.apply(lambda row: row[\"text\"][row[\"begin\"]:row[\"end\"]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with overlapping spans\n",
    "For now we just select the largest mention in an overlapping group and throw the others away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract overlapping spans\n",
    "conflicts = merge_with_spans(mentions, mentions, on=[\"doc_id\", (\"begin\", \"end\")], how=\"outer\", suffixes=(\"\", \"_other\"))\n",
    "\n",
    "# Assign a cluster (overlapping fragments) to each fragment\n",
    "mentions_cluster_ids = make_id_from_merged(\n",
    "    conflicts[[\"doc_id\", \"mention_id\"]], \n",
    "    conflicts[[\"doc_id\", \"mention_id_other\"]], \n",
    "    apply_on=[(0, mentions[[\"doc_id\", \"mention_id\"]])])\n",
    "\n",
    "mentions = (\n",
    "    mentions\n",
    "    .groupby(mentions_cluster_ids, as_index=False, observed=True, group_keys=False)\n",
    "    .apply(lambda group: group.assign(depth=np.argsort(group[\"begin\"]-group[\"end\"])))\n",
    "    .query('depth == 0').drop(columns=[\"depth\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize documents and mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlstruct.chunking.spacy_tokenization import spacy_tokenize, SPACY_ATTRIBUTES\n",
    "from nlstruct.core.text import split_into_spans, encode_as_tag\n",
    "\n",
    "# Tokenize\n",
    "tokens = spacy_tokenize(docs, lang=\"en_core_web_sm\", spacy_attributes=[\"norm_\"])\n",
    "mentions = split_into_spans(mentions, tokens, pos_col=\"token_idx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5590/5590 [00:05<00:00, 1006.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# Encode each mention as a BIO tag on its tokens\n",
    "tokens, label_categories = encode_as_tag(tokens, mentions, tag_scheme=\"bio\", use_token_idx=True, verbose=1, label_cols=[\"category\"])\n",
    "tokens.rename({\"category\": \"tag\"}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute vocabularies and prepare batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlstruct.core.batcher import Batcher\n",
    "from nlstruct.core.pandas import factorize_rows, normalize_vocabularies, df_to_csr, df_to_flatarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute idx for each mention in its doc according to its begin indice\n",
    "mentions = mentions.groupby('doc_id', as_index=False).apply(lambda group: group.assign(idx=np.argsort(np.argsort(group['begin'])))).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will train vocabulary for token_norm\n",
      "Will train vocabulary for tag\n",
      "Will train vocabulary for char\n",
      "Discovered existing vocabulary (9 entities) for tag\n",
      "Normalized category, with given vocabulary and no unk\n"
     ]
    }
   ],
   "source": [
    "# Construct charset from token->token_norm\n",
    "tokens[\"token_charset_id\"] = tokens[\"token_norm\"]\n",
    "charsets = (\n",
    "    tokens[['token_norm', 'token_charset_id']]\n",
    "    .drop_duplicates().astype(str)\n",
    "    .apply(lambda x: pd.Series({\"char\": tuple(x[\"token_norm\"]), \"token_charset_id\": x[\"token_charset_id\"]}, name=x.name), axis=1)\n",
    "    .nlstruct.flatten(\"char_idx\", tile_index=True)\n",
    ")\n",
    "[tokens[\"doc_id\"], mentions[\"doc_id\"]], unique_doc_ids = factorize_rows([tokens[\"doc_id\"], mentions[\"doc_id\"]])\n",
    "[mentions[\"mention_id\"]], unique_mention_ids = factorize_rows([mentions[\"mention_id\"]])\n",
    "[charsets[\"token_charset_id\"], tokens[\"token_charset_id\"]], unique_charset_ids = factorize_rows([charsets[\"token_charset_id\"], tokens[\"token_charset_id\"]])\n",
    "\n",
    "unk = {\n",
    "    \"token_norm\": \"<unk>\",\n",
    "    \"char\": \"<unk>\",\n",
    "}\n",
    "[tokens, mentions, charsets], vocabularies = normalize_vocabularies([tokens, mentions, charsets], vocabularies=label_categories, train_vocabularies=True, unk=unk, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batcher = Batcher({\n",
    "    \"doc\": {\n",
    "        \"token_norm\": df_to_csr(tokens[\"doc_id\"], tokens[\"token_idx\"], tokens[\"token_norm\"].cat.codes, n_rows=len(unique_doc_ids)),\n",
    "        \"token_charset_id\": df_to_csr(tokens[\"doc_id\"], tokens[\"token_idx\"], tokens[\"token_charset_id\"], n_rows=len(unique_doc_ids)),\n",
    "        \"token_tag\": df_to_csr(tokens[\"doc_id\"], tokens[\"token_idx\"], tokens[\"tag\"].cat.codes, n_rows=len(unique_doc_ids)),\n",
    "        \"token_mask\": df_to_csr(tokens[\"doc_id\"], tokens[\"token_idx\"], n_rows=len(unique_doc_ids)),\n",
    "        \"mention_id\": df_to_csr(mentions[\"doc_id\"], mentions[\"idx\"], mentions[\"mention_id\"], n_rows=len(unique_doc_ids)),\n",
    "        \"mention_mask\": df_to_csr(mentions[\"doc_id\"], mentions[\"idx\"], n_rows=len(unique_doc_ids)),\n",
    "    },\n",
    "    \"token_charset\": {\n",
    "        \"char\": df_to_csr(charsets[\"token_charset_id\"], charsets[\"char_idx\"], charsets[\"char\"].cat.codes),\n",
    "        \"mask\": df_to_csr(charsets[\"token_charset_id\"], charsets[\"char_idx\"]),\n",
    "    },\n",
    "    \"mention\": {\n",
    "        \"mention_id\": mentions[\"mention_id\"],\n",
    "        \"doc_id\": mentions[\"doc_id\"],\n",
    "        \"begin\": mentions[\"begin\"],\n",
    "        \"end\": mentions[\"end\"],\n",
    "        \"category\": mentions[\"category\"].cat.codes,\n",
    "    }}, \n",
    "    masks={\"doc\": {\"token_charset_id\": \"token_mask\", \"token_norm\": \"token_mask\", \"token_tag\": \"token_mask\", \"mention_id\": \"mention_mask\"}, \n",
    "           \"token_charset\": {\"char\": \"mask\"}}, \n",
    "    foreign_ids=\"absolute\").prepare_for_indexing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from nlstruct.layers.crf import BIODecoder\n",
    "from nlstruct.core.torch import torch_global as tg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character level encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharNet(torch.nn.Module):\n",
    "    def __init__(self, n_chars, dim, n_filters=128, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.embeddings = torch.nn.Embedding(n_chars, dim)\n",
    "        self.cnn = torch.nn.Conv1d(dim, n_filters, kernel_size=kernel_size)\n",
    "        \n",
    "    def forward(self, chars, mask):\n",
    "        # chars shape: n * n_char_per_token * dim\n",
    "        # mask shape: n * n_char_per_toke\n",
    "        state = self.embeddings(chars).transpose(1, 2)\n",
    "        # shape: n * n_char_per_token * dim\n",
    "        state = self.cnn(state)\n",
    "        # shape: n * n_filters * n_windows (n_windows ~= n_char_per_token)\n",
    "        state = F.relu(state).max(dim=2)[0]\n",
    "        # shape: n * n_filters\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERNet(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_chars, \n",
    "                 n_tokens, \n",
    "                 n_labels, \n",
    "                 token_dim, \n",
    "                 char_dim, \n",
    "                 char_hidden_dim, \n",
    "                 hidden_dim, \n",
    "                 rnn_layers, \n",
    "                 dropout, \n",
    "                 char_kernel_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = torch.nn.Embedding(n_tokens, token_dim) if n_tokens > 0 else None\n",
    "        self.char_net = CharNet(n_chars, char_dim, char_hidden_dim, kernel_size=char_kernel_size) if n_chars > 0 else None\n",
    "        assert self.embeddings is not None or self.char_net is not None\n",
    "        \n",
    "        self.crf = BIODecoder(n_labels, with_start_end_transitions=False)\n",
    "        self.lstm = torch.nn.LSTM((token_dim if n_tokens > 0 else 0) + (char_hidden_dim if n_chars > 0 else 0), \n",
    "                                  hidden_dim, dropout=dropout, batch_first=True, num_layers=rnn_layers, bidirectional=True)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.linear1 = torch.nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        self.linear2tag = torch.nn.Linear(hidden_dim, self.crf.num_tags)\n",
    "    \n",
    "    def forward(self, tokens, mask, charsets, charsets_mask, tokens_charset, tags=None, return_loss=False, return_argmax=False, reduction=\"mean\"):\n",
    "        # Embed the tokens\n",
    "        state = torch.cat([\n",
    "            *((self.embeddings(tokens).masked_fill(~mask.unsqueeze(-1), 0),) if self.embeddings is not None else ()),\n",
    "            *((self.char_net(charsets, charsets_mask)[tokens_charset],) if self.char_net is not None else ()),\n",
    "        ], dim=-1)\n",
    "        \n",
    "        # Run the lstm (first sort the sentences, pack them accorindg to the mask, unpack them and finally reorder them)\n",
    "        sorter = torch.argsort(-mask.sum(1))\n",
    "        invsorter = torch.argsort(sorter)\n",
    "        state = pad_packed_sequence(\n",
    "            self.lstm(pack_padded_sequence(state[sorter], mask[sorter].sum(1), batch_first=True))[0], \n",
    "            batch_first=True)[0].view(*state.shape[:2], -1)[invsorter]\n",
    "        \n",
    "        # Compute the tags scores\n",
    "        state = F.relu(self.linear1(self.dropout(state)))\n",
    "        state = self.linear2tag(state)\n",
    "        \n",
    "        return {\n",
    "            # Run the linear CRF forward algorithm on the tokens to compute the loglikelihood of the targets\n",
    "            \"loss\": -self.crf(state, mask, tags, reduction=reduction) if return_loss else None, \n",
    "            # Run the linear CRF Viterbi algorithm to compute the most likely sequence\n",
    "            \"pred\": self.crf.decode(state, mask) if return_argmax else None\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def tags_to_mentions(cls, pred_batch):\n",
    "        tag = pred_batch[\"doc\", \"token_tag\"].masked_fill(~pred_batch[\"doc\", \"token_mask\"], 0)\n",
    "        is_B = ((tag - 1) % 2) == 0\n",
    "        is_I = ((tag - 1) % 2 == 1) & (tag != 0)\n",
    "        label = (tag - 1) // 2\n",
    "        next_label = label.roll(-1, dims=1)\n",
    "        next_label[:, -1] = 0\n",
    "        next_I = is_I.roll(-1, dims=1)\n",
    "        next_I[:, -1] = 0\n",
    "        begin_tag = is_B.nonzero()\n",
    "        next_tag = tag.roll(-1, dims=1)\n",
    "        next_tag[:, -1] = 0\n",
    "        end_tag = (((tag != next_tag) & is_I) | (is_B & ~((label == next_label) & next_I))).nonzero()\n",
    "\n",
    "        mention_label = label[is_B]\n",
    "        mentions_count_per_doc = is_B.sum(-1)\n",
    "        max_mentions_count_per_doc = 0 if 0 in begin_tag.shape else mentions_count_per_doc.max()\n",
    "\n",
    "        doc_entity_id = torch.zeros(*mentions_count_per_doc.shape, max_mentions_count_per_doc, dtype=torch.long)\n",
    "        doc_entity_mask = torch.arange(max_mentions_count_per_doc, device=tag.device).view(1, -1) < mentions_count_per_doc.unsqueeze(-1)\n",
    "        doc_entity_id[doc_entity_mask] = torch.arange(begin_tag.shape[0], device=tag.device)\n",
    "\n",
    "        return Batcher({\n",
    "            \"doc\": {\n",
    "                \"doc_id\": pred_batch[\"doc\", \"doc_id\"],\n",
    "                \"mention_id\": doc_entity_id,\n",
    "                \"mention_mask\": doc_entity_mask,\n",
    "            },\n",
    "            \"mention\": {\n",
    "                \"begin\": begin_tag[:, 1],\n",
    "                \"end\": end_tag[:, 1] + 1, # for a tag sequence O O B I O, (begin_tag, end_tag) is (2, 3) so we want token span 2:4 \n",
    "                \"category\": mention_label,\n",
    "                \"doc_id\": begin_tag[:, 0],\n",
    "            }},\n",
    "            masks={\"doc\": {\"mention_id\": \"mention_mask\"}},\n",
    "            main_table=\"doc\",\n",
    "            foreign_ids=\"relative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Define the training metrics\n",
    "metrics_info = defaultdict(lambda: False)\n",
    "flt_format = (5, \"{:.4f}\".format)\n",
    "metrics_info.update({\n",
    "    \"train_loss\": {\"goal\": 0, \"format\": flt_format},\n",
    "    \"train_acc\": {\"goal\": 1, \"format\": flt_format},\n",
    "    \"val_loss\": {\"goal\": 0, \"format\": flt_format},\n",
    "    \"val_acc\": {\"goal\": 1, \"format\": flt_format},\n",
    "    \"val_recall\": {\"goal\": 1, \"format\": flt_format, \"name\": \"val_rec\"},\n",
    "    \"val_precision\": {\"goal\": 1, \"format\": flt_format, \"name\": \"val_prec\"},\n",
    "    \"val_f1\": {\"goal\": 1, \"format\": flt_format, \"name\": \"val_f1\"},\n",
    "    \"duration\": {\"format\": flt_format, \"name\": \"   dur(s)\"},\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache /Users/perceval/Development/data/cache/end2end_ner/e5095274fd12bedd\n",
      "\n",
      "\n",
      "epoch | train_loss | val_loss | val_rec | val_prec | \u001b[31mval_f1\u001b[0m |    dur(s)\n",
      "    1 |    \u001b[32m14.4776\u001b[0m |  \u001b[32m10.4004\u001b[0m |  \u001b[32m0.0000\u001b[0m |   \u001b[32m0.0000\u001b[0m | \u001b[32m0.0000\u001b[0m |   42.1471\n",
      "    2 |     \u001b[32m9.3936\u001b[0m |   \u001b[32m7.9732\u001b[0m |  \u001b[31m0.0000\u001b[0m |   \u001b[31m0.0000\u001b[0m | \u001b[31m0.0000\u001b[0m |   42.2215\n",
      "    3 |     \u001b[32m6.9858\u001b[0m |   \u001b[32m6.3759\u001b[0m |  \u001b[32m0.1583\u001b[0m |   \u001b[32m0.6423\u001b[0m | \u001b[32m0.2540\u001b[0m |   39.0092\n",
      "    4 |     \u001b[32m5.8309\u001b[0m |   \u001b[32m5.4808\u001b[0m |  \u001b[32m0.3246\u001b[0m |   \u001b[31m0.5997\u001b[0m | \u001b[32m0.4212\u001b[0m |   39.7426\n",
      "    5 |     \u001b[32m4.7964\u001b[0m |   \u001b[32m4.6218\u001b[0m |  \u001b[32m0.4290\u001b[0m |   \u001b[31m0.5566\u001b[0m | \u001b[32m0.4845\u001b[0m |   36.7979\n",
      "    6 |     \u001b[32m4.0481\u001b[0m |   \u001b[32m4.0535\u001b[0m |  \u001b[32m0.5207\u001b[0m |   \u001b[31m0.5589\u001b[0m | \u001b[32m0.5391\u001b[0m |   39.4023\n",
      "    7 |     \u001b[32m3.3773\u001b[0m |   \u001b[32m3.5485\u001b[0m |  \u001b[32m0.5737\u001b[0m |   \u001b[32m0.6831\u001b[0m | \u001b[32m0.6237\u001b[0m |   36.3798\n",
      "    8 |     \u001b[32m2.9540\u001b[0m |   \u001b[32m3.0442\u001b[0m |  \u001b[32m0.6538\u001b[0m |   \u001b[31m0.6763\u001b[0m | \u001b[32m0.6648\u001b[0m |   38.2260\n",
      "    9 |     \u001b[32m2.4959\u001b[0m |   \u001b[32m2.7114\u001b[0m |  \u001b[32m0.6862\u001b[0m |   \u001b[32m0.7365\u001b[0m | \u001b[32m0.7104\u001b[0m |   36.5517\n",
      "   10 |     \u001b[32m2.1421\u001b[0m |   \u001b[31m3.1543\u001b[0m |  \u001b[31m0.6538\u001b[0m |   \u001b[32m0.7989\u001b[0m | \u001b[32m0.7191\u001b[0m |   37.6450\n",
      "   11 |     \u001b[32m1.9474\u001b[0m |   \u001b[32m2.5217\u001b[0m |  \u001b[32m0.7617\u001b[0m |   \u001b[31m0.7302\u001b[0m | \u001b[32m0.7456\u001b[0m |   36.6518\n",
      "   12 |     \u001b[32m1.6490\u001b[0m |   \u001b[32m2.3223\u001b[0m |  \u001b[32m0.7626\u001b[0m |   \u001b[31m0.7845\u001b[0m | \u001b[32m0.7734\u001b[0m |   37.6477\n",
      "   13 |     \u001b[32m1.4736\u001b[0m |   \u001b[31m2.3635\u001b[0m |  \u001b[32m0.7662\u001b[0m |   \u001b[31m0.7926\u001b[0m | \u001b[32m0.7791\u001b[0m |   36.6118\n",
      "   14 |     \u001b[32m1.2908\u001b[0m |   \u001b[32m2.1634\u001b[0m |  \u001b[32m0.8112\u001b[0m |   \u001b[31m0.7618\u001b[0m | \u001b[32m0.7857\u001b[0m |   39.9254\n",
      "   15 |     \u001b[32m1.1832\u001b[0m |   \u001b[31m2.6307\u001b[0m |  \u001b[31m0.7608\u001b[0m |   \u001b[32m0.8368\u001b[0m | \u001b[32m0.7970\u001b[0m |   40.8993\n",
      "   16 |     \u001b[32m1.0660\u001b[0m |   \u001b[31m2.2136\u001b[0m |  \u001b[31m0.8058\u001b[0m |   \u001b[31m0.7818\u001b[0m | \u001b[31m0.7936\u001b[0m |   37.1553\n",
      "   17 |     \u001b[32m0.9317\u001b[0m |   \u001b[31m2.2053\u001b[0m |  \u001b[32m0.8165\u001b[0m |   \u001b[31m0.7754\u001b[0m | \u001b[31m0.7954\u001b[0m |   38.3252\n",
      "   18 |     \u001b[32m0.9255\u001b[0m |   \u001b[31m2.2262\u001b[0m |  \u001b[32m0.8246\u001b[0m |   \u001b[31m0.7946\u001b[0m | \u001b[32m0.8094\u001b[0m |   38.6141\n",
      "   19 |     \u001b[32m0.7581\u001b[0m |   \u001b[31m2.2789\u001b[0m |  \u001b[31m0.8129\u001b[0m |   \u001b[31m0.8174\u001b[0m | \u001b[32m0.8151\u001b[0m |   35.5671\n",
      "   20 |     \u001b[32m0.6949\u001b[0m |   \u001b[31m2.4168\u001b[0m |  \u001b[31m0.8121\u001b[0m |   \u001b[31m0.8005\u001b[0m | \u001b[31m0.8063\u001b[0m |   40.3759\n",
      "   21 |     \u001b[32m0.6436\u001b[0m |   \u001b[31m2.5055\u001b[0m |  \u001b[31m0.7968\u001b[0m |   \u001b[31m0.8158\u001b[0m | \u001b[31m0.8062\u001b[0m |   39.4160\n",
      "   22 |     \u001b[32m0.5562\u001b[0m |   \u001b[31m2.5357\u001b[0m |  \u001b[32m0.8300\u001b[0m |   \u001b[31m0.7815\u001b[0m | \u001b[31m0.8051\u001b[0m |   42.9061\n",
      "   23 |     \u001b[31m0.5847\u001b[0m |   \u001b[31m2.3952\u001b[0m |  \u001b[32m0.8408\u001b[0m |   \u001b[31m0.8019\u001b[0m | \u001b[32m0.8209\u001b[0m |   40.1311\n",
      "   24 |     \u001b[32m0.4977\u001b[0m |   \u001b[31m2.5797\u001b[0m |  \u001b[31m0.8309\u001b[0m |   \u001b[31m0.7931\u001b[0m | \u001b[31m0.8116\u001b[0m |   41.5082\n",
      "   25 |     \u001b[32m0.4091\u001b[0m |   \u001b[31m2.8150\u001b[0m |  \u001b[31m0.8183\u001b[0m |   \u001b[31m0.8356\u001b[0m | \u001b[32m0.8269\u001b[0m |   41.3735\n",
      "   26 |     \u001b[31m0.4216\u001b[0m |   \u001b[31m2.7990\u001b[0m |  \u001b[31m0.8318\u001b[0m |   \u001b[31m0.8057\u001b[0m | \u001b[31m0.8186\u001b[0m |   36.9758\n",
      "   27 |     \u001b[32m0.3953\u001b[0m |   \u001b[31m2.8193\u001b[0m |  \u001b[31m0.8309\u001b[0m |   \u001b[31m0.7918\u001b[0m | \u001b[31m0.8109\u001b[0m |   37.5929\n",
      "   28 |     \u001b[32m0.3941\u001b[0m |   \u001b[31m2.8191\u001b[0m |  \u001b[31m0.8282\u001b[0m |   \u001b[31m0.8179\u001b[0m | \u001b[31m0.8231\u001b[0m |   37.9381\n",
      "   29 |     \u001b[32m0.3411\u001b[0m |   \u001b[31m2.8918\u001b[0m |  \u001b[31m0.8246\u001b[0m |   \u001b[31m0.7974\u001b[0m | \u001b[31m0.8108\u001b[0m |   36.8037\n",
      "   30 |     \u001b[32m0.2947\u001b[0m |   \u001b[31m2.6490\u001b[0m |  \u001b[31m0.8264\u001b[0m |   \u001b[31m0.8154\u001b[0m | \u001b[31m0.8209\u001b[0m |   37.2403\n",
      "   31 |     \u001b[31m0.2953\u001b[0m |   \u001b[31m2.8900\u001b[0m |  \u001b[31m0.8327\u001b[0m |   \u001b[31m0.8159\u001b[0m | \u001b[31m0.8242\u001b[0m |   37.7301\n",
      "   32 |     \u001b[32m0.2645\u001b[0m |   \u001b[31m3.2571\u001b[0m |  \u001b[31m0.8282\u001b[0m |   \u001b[31m0.8107\u001b[0m | \u001b[31m0.8194\u001b[0m |   37.0076\n",
      "Loading /Users/perceval/Development/data/cache/end2end_ner/e5095274fd12bedd/checkpoint-25.pt... Done\n",
      "Model restored to its best state: 25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': 0.40906055861469187,\n",
       " 'val_loss': 2.81496830551933,\n",
       " 'val_recall': 0.8183453237410072,\n",
       " 'val_precision': 0.8356290174471993,\n",
       " 'val_f1': 0.8268968650613359,\n",
       " 'duration': 41.373486042022705,\n",
       " 'best_epoch': 25}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "from nlstruct.core.cache import get_cache\n",
    "from nlstruct.xp_helpers import make_optimizer_and_schedules, run_optimization\n",
    "from nlstruct.core.random import seed_all\n",
    "from nlstruct.core.torch import evaluating\n",
    "from nlstruct.core.scoring import compute_metrics, merge_pred_and_gold\n",
    "\n",
    "device = torch.device('cpu')\n",
    "seed = 42\n",
    "seed_all(seed) # /!\\ Super important to enable reproducibility\n",
    "\n",
    "# Split into train and val\n",
    "train_val_splits = np.random.choice([0, 1], size=len(batcher), p=[0.8, 0.2])\n",
    "train_batcher = batcher[train_val_splits == 0]\n",
    "val_batcher = batcher[train_val_splits == 1]\n",
    "\n",
    "batch_size = 64\n",
    "ner_net = NERNet(\n",
    "    n_chars=len(vocabularies[\"char\"]),\n",
    "    n_tokens=len(vocabularies[\"token_norm\"]), \n",
    "    n_labels=len(vocabularies[\"category\"]), \n",
    "    \n",
    "    token_dim=50,\n",
    "    char_dim=16,\n",
    "    char_hidden_dim=64,\n",
    "    hidden_dim=100,\n",
    "    dropout=0.5,\n",
    "    rnn_layers=2,\n",
    "    char_kernel_size=3,\n",
    ").to(device)\n",
    "\n",
    "optim, schedules = make_optimizer_and_schedules(ner_net, Adam, {\n",
    "    \"lr\": 1e-3,\n",
    "}, [\".*\"], num_iter_per_epoch=(len(train_batcher) + 1) / batch_size)\n",
    "\n",
    "# To debug the training, we can just comment the \"def run_epoch()\" and execute the function body manually without changing anything to it\n",
    "def run_epoch():\n",
    "    total_train_loss = 0\n",
    "    total_train_acc = 0\n",
    "    total_train_size = 0\n",
    "\n",
    "    #################\n",
    "    # TRAINING STEP #\n",
    "    #################\n",
    "    for batch in train_batcher.set_main(\"doc\").dataloader(batch_size=batch_size, shuffle=True, device=device):\n",
    "        optim.zero_grad()\n",
    "        res = ner_net.forward(\n",
    "            tokens =         batch[\"doc\", \"token_norm\"],\n",
    "            mask =           batch[\"doc\", \"token_mask\"],\n",
    "            charsets =       batch[\"token_charset\", \"char\"],\n",
    "            charsets_mask =  batch[\"token_charset\", \"mask\"],\n",
    "            tokens_charset = batch[\"doc\", \"token_charset_id\"],\n",
    "            tags =           batch[\"doc\", \"token_tag\"],\n",
    "            return_loss=True)\n",
    "\n",
    "        # Perform optimization step\n",
    "        loss = res[\"loss\"]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        for schedule_name, schedule in schedules.items():\n",
    "            schedule.step()\n",
    "\n",
    "        total_train_loss += loss.item() * len(batch)\n",
    "        total_train_size += len(batch)\n",
    "\n",
    "    ###################\n",
    "    # VALIDATION STEP #\n",
    "    ###################\n",
    "    total_val_loss = 0\n",
    "    total_val_size = 0\n",
    "    pred_batches = []\n",
    "    with evaluating(ner_net): # eval mode, no dropout\n",
    "        with torch.no_grad(): # no gradients -> faster\n",
    "            for batch in val_batcher.set_main(\"doc\").dataloader(batch_size=batch_size, device=device):\n",
    "                res = ner_net.forward(\n",
    "                    tokens =         batch[\"doc\", \"token_norm\"],\n",
    "                    mask =           batch[\"doc\", \"token_mask\"],\n",
    "                    charsets =       batch[\"token_charset\", \"char\"],\n",
    "                    charsets_mask =  batch[\"token_charset\", \"mask\"],\n",
    "                    tokens_charset = batch[\"doc\", \"token_charset_id\"],\n",
    "                    tags =           batch[\"doc\", \"token_tag\"],\n",
    "                    return_argmax=True,\n",
    "                    return_loss=True,\n",
    "                )\n",
    "                total_val_loss += res['loss'].item() * len(batch)\n",
    "                total_val_size += len(batch)\n",
    "                pred_batch = ner_net.tags_to_mentions(Batcher({\n",
    "                    \"doc\": {\n",
    "                        \"doc_id\": batch[\"doc\", \"doc_id\"],\n",
    "                        \"token_mask\": batch[\"doc\", \"token_mask\"],\n",
    "                        \"token_tag\": res[\"pred\"],\n",
    "                    },\n",
    "                }))\n",
    "                pred_batches.append(pred_batch)\n",
    "    pred = Batcher.concat(pred_batches)\n",
    "\n",
    "    # Compute precision, recall and f1 on validation set\n",
    "    val_metrics = compute_metrics(merge_pred_and_gold(\n",
    "        pred=pd.DataFrame(dict(pred[{\"mention\": [\"doc_id\", \"begin\", \"end\", \"category\"]}])), \n",
    "        gold=pd.DataFrame(dict(val_batcher.switch_foreign_ids_mode(\"absolute\")[{\"mention\": [\"doc_id\", \"begin\", \"end\", \"category\"]}])), \n",
    "        span_policy='partial_strict',  # only partially match spans with strict bounds, we could also eval with 'exact' or 'partial'\n",
    "        on=[\"doc_id\", (\"begin\", \"end\"), \"category\"]), prefix='val_')[[\"val_recall\", \"val_precision\", \"val_f1\"]].to_dict()\n",
    "    return \\\n",
    "    {\n",
    "        \"train_loss\": total_train_loss / total_train_size,\n",
    "        \"val_loss\": total_val_loss / total_val_size,\n",
    "        **val_metrics,\n",
    "    }\n",
    "\n",
    "state = {\"ner_net\": ner_net, \"optim\": optim, \"schedules\": schedules}  # all we need to restart the training from a given epoch\n",
    "run_optimization(\n",
    "    main_score='val_f1',\n",
    "    metrics_info=metrics_info,\n",
    "    required_start_score=10.00,\n",
    "    patience_warmup=5,\n",
    "    patience_rate=0.01,\n",
    "    patience=10,\n",
    "    max_epoch=32,\n",
    "\n",
    "    state=state, \n",
    "    cache=get_cache(\"end2end_ner\", {\"seed\": seed, \"data\": batcher, **state}, loader=torch.load, dumper=torch.save),  # where to store the model (main name + hashed parameters)\n",
    "    epoch_fn=run_epoch,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and batch encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized token_norm, with given vocabulary and unk <unk>\n",
      "Normalized char, with given vocabulary and unk <unk>\n"
     ]
    }
   ],
   "source": [
    "docs = test_dataset[\"docs\"].copy()\n",
    "\n",
    "# Clean the text / perform substitutions\n",
    "docs, deltas = transform_text(docs, *zip(*subs), return_deltas=True)\n",
    "\n",
    "# Tokenize\n",
    "tokens = spacy_tokenize(docs, lang=\"en_core_web_sm\", spacy_attributes=[\"norm_\"])\n",
    "\n",
    "def make_batcher(tokens):\n",
    "    tokens = tokens.copy()\n",
    "    tokens[\"token_charset_id\"] = tokens[\"token_norm\"]\n",
    "    charsets = (\n",
    "    tokens[['token_norm', 'token_charset_id']]\n",
    "        .drop_duplicates().astype(str)\n",
    "        .apply(lambda x: pd.Series({\"char\": tuple(x[\"token_norm\"]), \"token_charset_id\": x[\"token_charset_id\"]}, name=x.name), axis=1)\n",
    "        .nlstruct.flatten(\"char_idx\", tile_index=True)\n",
    "    )\n",
    "    [tokens[\"doc_id\"]], unique_doc_ids = factorize_rows([tokens[\"doc_id\"]])\n",
    "    [charsets[\"token_charset_id\"], tokens[\"token_charset_id\"]], unique_charset_ids = factorize_rows([charsets[\"token_charset_id\"], tokens[\"token_charset_id\"]])\n",
    "    [tokens, charsets] = normalize_vocabularies([tokens, charsets], verbose=1, unk=unk, vocabularies=vocabularies, train_vocabularies=False)[0]\n",
    "\n",
    "    batcher = Batcher({\n",
    "        \"doc\": {\n",
    "            \"token_norm\": df_to_csr(tokens[\"doc_id\"], tokens[\"token_idx\"], tokens[\"token_norm\"].cat.codes, n_rows=len(unique_doc_ids)),\n",
    "            \"token_charset_id\": df_to_csr(tokens[\"doc_id\"], tokens[\"token_idx\"], tokens[\"token_charset_id\"], n_rows=len(unique_doc_ids)),\n",
    "            \"token_mask\": df_to_csr(tokens[\"doc_id\"], tokens[\"token_idx\"], n_rows=len(unique_doc_ids)),\n",
    "        },\n",
    "        \"token_charset\": {\n",
    "            \"char\": df_to_csr(charsets[\"token_charset_id\"], charsets[\"char_idx\"], charsets[\"char\"].cat.codes),\n",
    "            \"mask\": df_to_csr(charsets[\"token_charset_id\"], charsets[\"char_idx\"]),\n",
    "        }}, \n",
    "        masks={\n",
    "            \"doc\": {\"token_charset_id\": \"token_mask\", \"token_norm\": \"token_mask\"}, \n",
    "            \"token_charset\": {\"char\": \"mask\"}}, \n",
    "        foreign_ids=\"absolute\",\n",
    "    ).prepare_for_indexing()\n",
    "    return batcher, unique_doc_ids\n",
    "\n",
    "batcher, unique_doc_ids = make_batcher(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "pred_batches = []\n",
    "with evaluating(ner_net): # eval mode, no dropout\n",
    "    with torch.no_grad(): # no gradients -> faster\n",
    "        for batch in batcher.set_main(\"doc\").dataloader(batch_size=128, device=device):\n",
    "            res = ner_net.forward(\n",
    "                tokens =         batch[\"doc\", \"token_norm\"],\n",
    "                mask =           batch[\"doc\", \"token_mask\"],\n",
    "                charsets =       batch[\"token_charset\", \"char\"],\n",
    "                charsets_mask =  batch[\"token_charset\", \"mask\"],\n",
    "                tokens_charset = batch[\"doc\", \"token_charset_id\"],\n",
    "                return_argmax=True,\n",
    "            )\n",
    "            pred_batches.append(ner_net.tags_to_mentions(Batcher({\n",
    "                \"doc\": {\n",
    "                    \"doc_id\": batch[\"doc\", \"doc_id\"],\n",
    "                    \"token_mask\": batch[\"doc\", \"token_mask\"],\n",
    "                    \"token_tag\": res[\"pred\"],\n",
    "                },\n",
    "            })))\n",
    "pred = Batcher.concat(pred_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert back to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlstruct.core.text import reverse_deltas\n",
    "from nlstruct.core.dataset import Dataset\n",
    "\n",
    "# Convert predicted concat mention batches to mention df\n",
    "pred_mentions = pd.DataFrame({\n",
    "    \"doc_id\": unique_doc_ids.iloc[pred[\"mention\", \"doc_id\"]],\n",
    "    \"begin\": pred[\"mention\", \"begin\"],\n",
    "    \"end\": pred[\"mention\", \"end\"] - 1, # if token span is 2:4, the last token is at position 3\n",
    "    \"category\": np.asarray(vocabularies[\"category\"])[pred[\"mention\", \"category\"]],\n",
    "}).groupby(\"doc_id\", as_index=False, observed=True).apply(lambda x: x.assign(mention_id=[\"T\"+str(i+1) for i in np.argsort(np.argsort(x[\"begin\"].values))])).reset_index(drop=True)\n",
    "\n",
    "# Convert token spans (mentions) to character spans\n",
    "pred_mentions = pd.merge(pred_mentions, tokens[[\"doc_id\", \"token_idx\", \"begin\"]],\n",
    "                         left_on=['doc_id', 'begin'], right_on=['doc_id', 'token_idx'], suffixes=('_x', '')).drop(columns=[\"token_idx\"])\n",
    "pred_mentions = pd.merge(pred_mentions, tokens[[\"doc_id\", \"token_idx\", \"end\"]],\n",
    "                         left_on=['doc_id', 'end'], right_on=['doc_id', 'token_idx'], suffixes=('_x', '')).drop(columns=[\"token_idx\"])\n",
    "\n",
    "# Apply the reverse text transformations on the mentions\n",
    "pred_mentions = reverse_deltas(pred_mentions.drop(columns=[\"begin_x\", \"end_x\"]), deltas, on=[\"doc_id\"])\n",
    "pred_dataset = Dataset(\n",
    "    docs=test_dataset[\"docs\"],\n",
    "    mentions=pred_mentions[[\"doc_id\", \"mention_id\", \"category\"]],\n",
    "    fragments=pred_mentions[[\"doc_id\", \"mention_id\", \"begin\", \"end\"]].assign(fragment_id=0),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>gold_count</th>\n",
       "      <th>precision</th>\n",
       "      <th>pred_count</th>\n",
       "      <th>recall</th>\n",
       "      <th>tp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">full</th>\n",
       "      <th>exact</th>\n",
       "      <td>0.602572</td>\n",
       "      <td>1291.0</td>\n",
       "      <td>0.515118</td>\n",
       "      <td>1819.0</td>\n",
       "      <td>0.725794</td>\n",
       "      <td>937.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partial_strict</th>\n",
       "      <td>0.663023</td>\n",
       "      <td>1291.0</td>\n",
       "      <td>0.566795</td>\n",
       "      <td>1819.0</td>\n",
       "      <td>0.798606</td>\n",
       "      <td>1031.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">span_only</th>\n",
       "      <th>exact</th>\n",
       "      <td>0.677814</td>\n",
       "      <td>1291.0</td>\n",
       "      <td>0.579439</td>\n",
       "      <td>1819.0</td>\n",
       "      <td>0.816421</td>\n",
       "      <td>1054.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partial_strict</th>\n",
       "      <td>0.769132</td>\n",
       "      <td>1291.0</td>\n",
       "      <td>0.657504</td>\n",
       "      <td>1819.0</td>\n",
       "      <td>0.926414</td>\n",
       "      <td>1196.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                f1  gold_count  precision  pred_count  \\\n",
       "full      exact           0.602572      1291.0   0.515118      1819.0   \n",
       "          partial_strict  0.663023      1291.0   0.566795      1819.0   \n",
       "span_only exact           0.677814      1291.0   0.579439      1819.0   \n",
       "          partial_strict  0.769132      1291.0   0.657504      1819.0   \n",
       "\n",
       "                            recall      tp  \n",
       "full      exact           0.725794   937.0  \n",
       "          partial_strict  0.798606  1031.0  \n",
       "span_only exact           0.816421  1054.0  \n",
       "          partial_strict  0.926414  1196.0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_mentions = test_dataset[\"mentions\"].merge(test_dataset[\"fragments\"], on=[\"doc_id\", \"mention_id\"])\n",
    "pred_mentions = pred_dataset[\"mentions\"].merge(pred_dataset[\"fragments\"], on=[\"doc_id\", \"mention_id\"])\n",
    "val_metrics = {\n",
    "    **compute_metrics(merge_pred_and_gold(\n",
    "        pred=pred_mentions, atom_pred_level=[\"doc_id\", \"mention_id\"],\n",
    "        gold=gold_mentions, atom_gold_level=[\"doc_id\", \"mention_id\"],\n",
    "        span_policy='partial_strict',  # only partially match spans with strict bounds, we could also eval with 'exact' or 'partial'\n",
    "        on=[\"doc_id\", (\"begin\", \"end\"), \"category\"]), prefix=\"full/partial_strict/\"),\n",
    "    **compute_metrics(merge_pred_and_gold(\n",
    "        pred=pred_mentions, atom_pred_level=[\"doc_id\", \"mention_id\"],\n",
    "        gold=gold_mentions, atom_gold_level=[\"doc_id\", \"mention_id\"],\n",
    "        span_policy='exact',  # only partially match spans with strict bounds, we could also eval with 'exact' or 'partial'\n",
    "        on=[\"doc_id\", (\"begin\", \"end\"), \"category\"]), prefix=\"full/exact/\"),\n",
    "    **compute_metrics(merge_pred_and_gold(\n",
    "        pred=pred_mentions, atom_pred_level=[\"doc_id\", \"mention_id\"],\n",
    "        gold=gold_mentions, atom_gold_level=[\"doc_id\", \"mention_id\"],\n",
    "        span_policy='partial_strict',  # only partially match spans with strict bounds, we could also eval with 'exact' or 'partial'\n",
    "        on=[\"doc_id\", (\"begin\", \"end\")]), prefix=\"span_only/partial_strict/\"),\n",
    "    **compute_metrics(merge_pred_and_gold(\n",
    "        pred=pred_mentions, atom_pred_level=[\"doc_id\", \"mention_id\"],\n",
    "        gold=gold_mentions, atom_gold_level=[\"doc_id\", \"mention_id\"],\n",
    "        span_policy='exact',  # only partially match spans with strict bounds, we could also eval with 'exact' or 'partial'\n",
    "        on=[\"doc_id\", (\"begin\", \"end\")]), prefix=\"span_only/exact/\"),\n",
    "}\n",
    "pd.Series({tuple(name.split(\"/\")): value for name, value in val_metrics.items()}).unstack(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export / visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2 style=\"margin: 0\">100562</h2>\n",
       "\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Familial deficiency of the seventh component of complement\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SpecificDisease</span>\n",
       "</mark>\n",
       " associated with recurrent \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    bacteremic infections\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DiseaseClass</span>\n",
       "</mark>\n",
       " due to \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Neisseria\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DiseaseClass</span>\n",
       "</mark>\n",
       ".</br>The serum of a 29-year old woman with a recent episode of disseminated \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    gonococcal infection\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SpecificDisease</span>\n",
       "</mark>\n",
       " and a history of \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    meningococcal meningitis\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SpecificDisease</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    arthritis\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SpecificDisease</span>\n",
       "</mark>\n",
       " as a child was found to lack serum hemolytic complement activity. The \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    seventh component of complement\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SpecificDisease</span>\n",
       "</mark>\n",
       " (C7) was not detected by functional or immunochemical assays, whereas other components were normal by hemolytic and immunochemical assessment. Her fresh serum lacked complement-mediated bactericidal activity against \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Neisseria gonorrhoeae\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SpecificDisease</span>\n",
       "</mark>\n",
       ", but the addition of fresh normal serum or purified C7 restored bactericidal activity as well as hemolytic activity. The absence of functional C7 activity could not be accounted for on the basis of an inhibitor. Opsonization and generation of chemotactic activity functioned normally. Complete absence of C7 was also found in one sibling who had the clinical syndrome of \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    meningococcal meningitis\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SpecificDisease</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    arthritis\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SpecificDisease</span>\n",
       "</mark>\n",
       "  as a child and in this siblings clinically well eight-year-old son. HLA histocompatibility typing of the family members did not demonstrate evidence for genetic linkage of \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    C7 deficiency\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SpecificDisease</span>\n",
       "</mark>\n",
       " with the major histocompatibility loci. This report represents the first cases of \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    C7 deficiency\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SpecificDisease</span>\n",
       "</mark>\n",
       " associated with infectious complications and suggests that bactericidal activity may be important in host defense against \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    bacteremic neisseria infections\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SpecificDisease</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nlstruct.exporters.visualizers import render_with_displacy\n",
    "render_with_displacy(pred_dataset.query('doc_id == \"100562\"'), label_colname=\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2 style=\"margin: 0\">100562</h2>\n",
       "\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Familial deficiency of the seventh component of complement\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SpecificDisease</span>\n",
       "</mark>\n",
       " associated with recurrent \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    bacteremic infections due to Neisseria\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DiseaseClass</span>\n",
       "</mark>\n",
       ".</br>The serum of a 29-year old woman with a recent episode of \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    disseminated gonococcal infection\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SpecificDisease</span>\n",
       "</mark>\n",
       " and a history of \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    meningococcal meningitis\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SpecificDisease</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    arthritis\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DiseaseClass</span>\n",
       "</mark>\n",
       " as a child was found to lack serum hemolytic complement activity. The seventh component of complement (C7) was not detected by functional or immunochemical assays, whereas other components were normal by hemolytic and immunochemical assessment. Her fresh serum lacked complement-mediated bactericidal activity against Neisseria gonorrhoeae, but the addition of fresh normal serum or purified C7 restored bactericidal activity as well as hemolytic activity. The \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    absence of functional C7\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">Modifier</span>\n",
       "</mark>\n",
       " activity could not be accounted for on the basis of an inhibitor. Opsonization and generation of chemotactic activity functioned normally. \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Complete absence of C7\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SpecificDisease</span>\n",
       "</mark>\n",
       " was also found in one sibling who had the clinical syndrome of \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    meningococcal meningitis\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SpecificDisease</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    arthritis\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DiseaseClass</span>\n",
       "</mark>\n",
       "  as a child and in this siblings clinically well eight-year-old son. HLA histocompatibility typing of the family members did not demonstrate evidence for genetic linkage of \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    C7 deficiency\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SpecificDisease</span>\n",
       "</mark>\n",
       " with the major histocompatibility loci. This report represents the first cases of \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    C7 deficiency\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SpecificDisease</span>\n",
       "</mark>\n",
       " associated with infectious complications and suggests that bactericidal activity may be important in host defense against \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    bacteremic neisseria infections\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SpecificDisease</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nlstruct.exporters.visualizers import render_with_displacy\n",
    "render_with_displacy(test_dataset.query('doc_id == \"100562\"'), label_colname=\"category\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLStruct",
   "language": "python",
   "name": "nlstruct"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
